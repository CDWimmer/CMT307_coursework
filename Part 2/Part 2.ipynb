{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firstly import all the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import operator\n",
    "# from joblib import dump, load  # for saving the SVM to disk\n",
    "# import random\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "# from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (If required) download the required nltk files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/charlie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/charlie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/charlie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # If needed. Could optionally define from scratch. \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create various utility/convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(filename_pos: str, filename_neg: str, shuffle: bool = False):\n",
    "    \"\"\"Loads data into a combined pandas DataFrame. will randomly shuffle\n",
    "    the rows if specified. DataFrame will have columns as such:\n",
    "    ##################\n",
    "    \"review\" | \"sentiment\"\n",
    "    string   | int[0,1]\n",
    "    ...\n",
    "    ##################\n",
    "    Where a 1 in the sentiment column indicates a negative review.\n",
    "    \"\"\"\n",
    "    df_pos = pd.read_csv(filename_pos, sep=\"\\n\", header=None, names=['review'], encoding='utf8')\n",
    "    df_pos['sentiment'] = 0  # 0 = positive\n",
    "    df_neg = pd.read_csv(filename_neg, sep=\"\\n\", header=None, names=['review'], encoding='utf8')\n",
    "    df_neg['sentiment'] = 1  # 1 = negative\n",
    "    df = df_pos.append(df_neg)\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1, random_state=37).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "class chi2_feature_selection:\n",
    "    def __init__(self, X_training, Y_training, k_final_words):\n",
    "        self.fs_sentanalysis = SelectKBest(chi2, k=k_final_words).fit(X_training, Y_training)\n",
    "        \n",
    "    def do_selection(self, X_data):\n",
    "        X_data_chi2 = self.fs_sentanalysis.transform(X_data)\n",
    "        print(\"Size original matrix: \" + str(X_data.shape))\n",
    "        print(\"Size new matrix: \" + str(X_data_chi2.shape))\n",
    "        return X_data_chi2\n",
    "\n",
    "\n",
    "def get_score(Y_gold, Y_pred):\n",
    "    precision = precision_score(Y_gold, Y_pred, average='macro')\n",
    "    recall = recall_score(Y_gold, Y_pred, average='macro')\n",
    "    f1 = f1_score(Y_gold, Y_pred, average='macro')\n",
    "    accuracy = accuracy_score(Y_gold, Y_pred)\n",
    "    print(f\"Precision: {round(precision, 3)}\")\n",
    "    print(f\"Recall: {round(recall, 3)}\")\n",
    "    print(f\"F1-Score: {round(f1, 3)}\")\n",
    "    print(f\"Accuracy: {round(accuracy, 3)}\")\n",
    "    return {\"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define stopwords set\n",
    "\n",
    "Initially using the nltk built-in english stopwords, then adding in a customized list of stopwords - based on the output seen from the vectorizers later in this Notebook. E.g. the dataset includes some HTML tags like `<br>` which obviously aren't useful for sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "extra_stopwords = [\".\", \",\", \"--\", \"``\", \"''\", \"&\", \"!\", \"?\", \"'s\", \"n't\", \";\", \"'\", \"-\", \"/\", \"<\", \">\", \n",
    "                   \"br\", \"(\", \")\", \"wa\", \"ha\", \"...\", \":\", \"^\", \"`\", \"*\", \"\"]\n",
    "for stopword in extra_stopwords:\n",
    "    stopwords.add(stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train_set = load_data(\"IMDb/train/imdb_train_pos.txt\", \"IMDb/train/imdb_train_neg.txt\", True)  # choo choo\n",
    "dev_set = load_data(\"IMDb/dev/imdb_dev_pos.txt\", \"IMDb/dev/imdb_dev_neg.txt\", True)\n",
    "test_set = load_data(\"IMDb/test/imdb_test_pos.txt\", \"IMDb/test/imdb_test_neg.txt\", True)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and refine each feature's vectors:\n",
    "<ol>\n",
    "    <li>Create standard TF-IDF Vectors</li>\n",
    "    <li>Create TF-IDF bi-gram Vectors</li>\n",
    "        <li>combine 1 and 2 and perform chi2 feature selection to narrow it down to the 100 most significant features</li>\n",
    "    <li>Create character count Vectors - 1D vector so no selection required.</li>\n",
    "    <li>Combine all vectors</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF-IDF unigram vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating TF-IDF vectors...\n",
      "Done!\n",
      "produced 870 features, top 50:\n",
      "['10' '20' '30' '80' 'able' 'absolutely' 'across' 'act' 'acted' 'acting'\n",
      " 'action' 'actor' 'actors' 'actress' 'actual' 'actually' 'add' 'admit'\n",
      " 'age' 'ago' 'agree' 'air' 'alive' 'almost' 'alone' 'along' 'already'\n",
      " 'also' 'although' 'always' 'amazing' 'america' 'american' 'among'\n",
      " 'amount' 'annoying' 'another' 'anyone' 'anything' 'anyway' 'apart'\n",
      " 'apparently' 'appear' 'appears' 'around' 'art' 'ask' 'atmosphere'\n",
      " 'attempt' 'attempts']\n"
     ]
    }
   ],
   "source": [
    "# perform the TF-IDF count vectorization.\n",
    "print(\"Calculating TF-IDF vectors...\")\n",
    "tfidf_1 = TfidfVectorizer(stop_words=stopwords, max_df=0.9, min_df=0.02)\n",
    "tfidf_1.fit(train_set['review'], train_set['sentiment'])\n",
    "\n",
    "X_train_tfidf_1 = tfidf_1.transform(train_set['review'])\n",
    "X_dev_tfidf_1 = tfidf_1.transform(dev_set['review'])\n",
    "X_test_tfidf_1 = tfidf_1.transform(test_set['review'])\n",
    "print(\"Done!\")\n",
    "print(f\"produced {len(np.asarray(tfidf_1.get_feature_names()))} features, top 50:\\n{np.asarray(tfidf_1.get_feature_names())[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bigram TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating TF-IDF bi-gram vectors...\n",
      "Done!\n",
      "produced 133 features. \n"
     ]
    }
   ],
   "source": [
    "# Perform TF-IDF vectorization with bi-grams. \n",
    "print(\"Calculating TF-IDF bi-gram vectors...\")\n",
    "tfidf_2 = TfidfVectorizer(stop_words=stopwords, ngram_range=(2, 2), max_df=0.90, min_df=0.01)\n",
    "tfidf_2.fit(train_set['review'], train_set['sentiment'])\n",
    "\n",
    "X_train_tfidf_2 = tfidf_2.transform(train_set['review'])\n",
    "X_dev_tfidf_2 = tfidf_2.transform(dev_set['review'])\n",
    "X_test_tfidf_2 = tfidf_2.transform(test_set['review'])\n",
    "print(\"Done!\")\n",
    "print(f\"produced {len(np.asarray(tfidf_2.get_feature_names()))} features. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine above vectors\n",
    "X_train_combi = hstack([X_train_tfidf_1, X_train_tfidf_2])\n",
    "\n",
    "X_dev_combi = hstack([X_dev_tfidf_1, X_dev_tfidf_2])\n",
    "\n",
    "X_test_combi = hstack([X_test_tfidf_1, X_test_tfidf_2])\n",
    "\n",
    "#X_train_combi = X_train_tfidf_1\n",
    "#X_dev_combi = X_dev_tfidf_1\n",
    "#X_test_combi = X_test_tfidf_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform feature selection via SelectKBest using chi$^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size original matrix: (15000, 1003)\n",
      "Size new matrix: (15000, 200)\n",
      "Size original matrix: (5000, 1003)\n",
      "Size new matrix: (5000, 200)\n",
      "Size original matrix: (5000, 1003)\n",
      "Size new matrix: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "k = 200\n",
    "chi2_selector = chi2_feature_selection(X_train_combi, train_set[\"sentiment\"], k)\n",
    "\n",
    "X_train_chi2 = chi2_selector.do_selection(X_train_combi)\n",
    "X_dev_chi2 = chi2_selector.do_selection(X_dev_combi)\n",
    "X_test_chi2 = chi2_selector.do_selection(X_test_combi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector with the average sentence length and the word count of the reviews\n",
    "\n",
    "Being a 2-dimension vector of hand-picked features I don't believe this requires any feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count vector\n",
    "\n",
    "X_train_chars, X_dev_chars, X_test_chars = [], [], []\n",
    "for review in train_set[\"review\"]:\n",
    "    X_train_chars.append([np.mean([len(sent) for sent in review.split(\".\")]), np.mean(len(review.split(\" \")))])\n",
    "for review in dev_set[\"review\"]:\n",
    "    X_dev_chars.append([np.mean([len(sent) for sent in review.split(\".\")]), np.mean(len(review.split(\" \")))])\n",
    "for review in test_set[\"review\"]:\n",
    "    X_test_chars.append([np.mean([len(sent) for sent in review.split(\".\")]), np.mean(len(review.split(\" \")))])\n",
    "    \n",
    "# np.mean([len(sent) for sent in review.split(\".\")]), \n",
    "# Scale to a range of 0 to 1 to match the TfidfVectorizer output\n",
    "scaling = MinMaxScaler(feature_range=(0,1)).fit(X_train_chars)\n",
    "# Convert into sparse matrices to match as above\n",
    "X_train_chars = coo_matrix(scaling.transform(X_train_chars))\n",
    "X_dev_chars = coo_matrix(scaling.transform(X_dev_chars))\n",
    "X_test_chars = coo_matrix(scaling.transform(X_test_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dimensions of training data: (15000, 202)\n"
     ]
    }
   ],
   "source": [
    "# Combine into final input vectors\n",
    "\n",
    "X_train_final = hstack([X_train_chi2, X_train_chars])\n",
    "X_dev_final = hstack([X_dev_chi2, X_dev_chars])\n",
    "X_test_final = hstack([X_test_chi2, X_test_chars])\n",
    "\n",
    "print(f\"Final dimensions of training data: {X_train_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple classifiers with different settings\n",
    "(These steps may take a while)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Linear SVM training...\n",
      "Done! Time to train classifier was: 12.939 s\n"
     ]
    }
   ],
   "source": [
    "# Train Linear SVM Classifier\n",
    "print(\"Beginning Linear SVM training...\")\n",
    "start = time.time()\n",
    "svm_clf_lin = sklearn.svm.SVC(kernel='linear', gamma='auto')\n",
    "svm_clf_lin_result = svm_clf_lin.fit(X_train_final, train_set[\"sentiment\"])\n",
    "end = time.time()\n",
    "print(f\"Done! Time to train classifier was: {round(end - start, 3)} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial classifier - 3 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Polynomial SVM training...\n",
      "Done! Time to train classifier was: 24.954 s\n"
     ]
    }
   ],
   "source": [
    "# Train Poly SVM Classifier\n",
    "print(\"Beginning Polynomial SVM training...\")\n",
    "start = time.time()\n",
    "\n",
    "svm_clf_poly3 = sklearn.svm.SVC(kernel='poly', degree=3, coef0=1, gamma='auto')\n",
    "\n",
    "svm_clf_poly3_result = svm_clf_poly3.fit(X_train_final, train_set[\"sentiment\"])\n",
    "end = time.time()\n",
    "print(f\"Done! Time to train classifier was: {round(end - start, 3)} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial classifier - 6 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Polynomial SVM training...\n",
      "Done! Time to train classifier was: 22.101 s\n"
     ]
    }
   ],
   "source": [
    "# Train Poly SVM Classifier\n",
    "print(\"Beginning Polynomial SVM training...\")\n",
    "start = time.time()\n",
    "\n",
    "svm_clf_poly6 = sklearn.svm.SVC(kernel='poly', degree=6, coef0=1, gamma='auto')\n",
    "\n",
    "svm_clf_poly6_result = svm_clf_poly6.fit(X_train_final, train_set[\"sentiment\"])\n",
    "end = time.time()\n",
    "print(f\"Done! Time to train classifier was: {round(end - start, 3)} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial classifier - 10 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Polynomial SVM training...\n",
      "Done! Time to train classifier was: 19.975 s\n"
     ]
    }
   ],
   "source": [
    "# Train Poly SVM Classifier\n",
    "print(\"Beginning Polynomial SVM training...\")\n",
    "start = time.time()\n",
    "\n",
    "svm_clf_poly10 = sklearn.svm.SVC(kernel='poly', degree=10, coef0=1, gamma='auto')\n",
    "\n",
    "svm_clf_poly10_result = svm_clf_poly10.fit(X_train_final, train_set[\"sentiment\"])\n",
    "end = time.time()\n",
    "print(f\"Done! Time to train classifier was: {round(end - start, 3)} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial classifier - 15 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Polynomial SVM training...\n",
      "Done! Time to train classifier was: 18.126 s\n"
     ]
    }
   ],
   "source": [
    "# Train Poly SVM Classifier\n",
    "print(\"Beginning Polynomial SVM training...\")\n",
    "start = time.time()\n",
    "\n",
    "svm_clf_poly15 = sklearn.svm.SVC(kernel='poly', degree=15, coef0=1, gamma='auto')\n",
    "\n",
    "svm_clf_poly15_result = svm_clf_poly15.fit(X_train_final, train_set[\"sentiment\"])\n",
    "end = time.time()\n",
    "print(f\"Done! Time to train classifier was: {round(end - start, 3)} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning RBF SVM training...\n",
      "Done! Time to train classifier was: 28.835 s\n"
     ]
    }
   ],
   "source": [
    "# Train RBF SVM Classifier\n",
    "print(\"Beginning RBF SVM training...\")\n",
    "start = time.time()\n",
    "\n",
    "svm_clf_rbf = sklearn.svm.SVC(kernel='rbf', gamma='auto')\n",
    "svm_clf_rbf_result = svm_clf_rbf.fit(X_train_final, train_set[\"sentiment\"])\n",
    "end = time.time()\n",
    "print(f\"Done! Time to train classifier was: {round(end - start, 3)} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Decision Tree training...\n",
      "Done! Time to train classifier was: 0.109 s\n"
     ]
    }
   ],
   "source": [
    "# Train Decision Tree\n",
    "print(\"Beginning Decision Tree training...\")\n",
    "start = time.time()\n",
    "dtree_clf = DecisionTreeClassifier(max_depth=4, random_state=37)\n",
    "dtree_clf_result = dtree_clf.fit(X_train_final, train_set[\"sentiment\"])\n",
    "end = time.time()\n",
    "print(f\"Done! Time to train classifier was: {round(end - start, 3)} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions from models\n",
    "\n",
    "(Takes a few moments for each model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM:\n",
      "------------\n",
      "Precision: 0.84\n",
      "Recall: 0.839\n",
      "F1-Score: 0.839\n",
      "Accuracy: 0.839\n",
      "\n",
      "3 degree Polynomial SVM:\n",
      "------------\n",
      "Precision: 0.801\n",
      "Recall: 0.792\n",
      "F1-Score: 0.791\n",
      "Accuracy: 0.792\n",
      "\n",
      "6 degree Polynomial SVM:\n",
      "------------\n",
      "Precision: 0.815\n",
      "Recall: 0.809\n",
      "F1-Score: 0.809\n",
      "Accuracy: 0.81\n",
      "\n",
      "10 degree Polynomial SVM:\n",
      "------------\n",
      "Precision: 0.825\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "\n",
      "15 degree Polynomial SVM:\n",
      "------------\n",
      "Precision: 0.829\n",
      "Recall: 0.827\n",
      "F1-Score: 0.827\n",
      "Accuracy: 0.827\n",
      "\n",
      "RBF SVM:\n",
      "------------\n",
      "Precision: 0.787\n",
      "Recall: 0.775\n",
      "F1-Score: 0.774\n",
      "Accuracy: 0.776\n",
      "\n",
      "Decision Tree:\n",
      "------------\n",
      "Precision: 0.721\n",
      "Recall: 0.67\n",
      "F1-Score: 0.651\n",
      "Accuracy: 0.672\n"
     ]
    }
   ],
   "source": [
    "dev_predictions = {}\n",
    "dev_lin_pred = svm_clf_lin.predict(X_dev_final)\n",
    "print(\"Linear SVM:\\n------------\")\n",
    "dev_predictions.update( {\"linear\": get_score(dev_set[\"sentiment\"], dev_lin_pred)} )\n",
    "\n",
    "dev_poly3_pred = svm_clf_poly3.predict(X_dev_final)\n",
    "print(\"\\n3 degree Polynomial SVM:\\n------------\")\n",
    "dev_predictions.update( {\"poly3\": get_score(dev_set[\"sentiment\"], dev_poly3_pred)} )\n",
    "\n",
    "dev_poly6_pred = svm_clf_poly6.predict(X_dev_final)\n",
    "print(\"\\n6 degree Polynomial SVM:\\n------------\")\n",
    "dev_predictions.update( {\"poly6\": get_score(dev_set[\"sentiment\"], dev_poly6_pred)} )\n",
    "\n",
    "dev_poly10_pred = svm_clf_poly10.predict(X_dev_final)\n",
    "print(\"\\n10 degree Polynomial SVM:\\n------------\")\n",
    "dev_predictions.update( {\"poly10\": get_score(dev_set[\"sentiment\"], dev_poly10_pred)} )\n",
    "\n",
    "dev_poly15_pred = svm_clf_poly15.predict(X_dev_final)\n",
    "print(\"\\n15 degree Polynomial SVM:\\n------------\")\n",
    "dev_predictions.update( {\"poly15\": get_score(dev_set[\"sentiment\"], dev_poly15_pred)} )\n",
    "\n",
    "dev_rbf_pred = svm_clf_rbf.predict(X_dev_final)\n",
    "print(\"\\nRBF SVM:\\n------------\")\n",
    "dev_predictions.update( {\"RBF\": get_score(dev_set[\"sentiment\"], dev_rbf_pred)} )\n",
    "\n",
    "dev_dtree_pred = dtree_clf.predict(X_dev_final)\n",
    "print(\"\\nDecision Tree:\\n------------\")\n",
    "dev_predictions.update( {\"Decision tree\": get_score(dev_set[\"sentiment\"], dev_dtree_pred)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results for different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "my_xticks = [result for result in dev_predictions]\n",
    "x = np.arange(len(dev_predictions))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.xticks(x, my_xticks)\n",
    "\n",
    "y_precision = [dev_predictions[result][\"precision\"] for result in dev_predictions]\n",
    "y_recall = [dev_predictions[result][\"recall\"] for result in dev_predictions]\n",
    "y_f1 = [dev_predictions[result][\"f1_score\"] for result in dev_predictions]\n",
    "y_accuracy = [dev_predictions[result][\"accuracy\"] for result in dev_predictions]\n",
    "\n",
    "plt.plot(x, y_precision, marker='o', label=\"precision\", color=\"blue\", linestyle='none')\n",
    "plt.plot(x, y_recall, marker='o', label=\"recall\", color=\"orange\", linestyle='none')\n",
    "plt.plot(x, y_f1, marker='o', label=\"f1 score\", color=\"red\", linestyle='none')\n",
    "plt.plot(x, y_accuracy, marker='o', label=\"accuracy\", color=\"green\", linestyle='none')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "From the above we can see that the <b>linear model</b> is the best in terms of both time to train and every single statistic for the development set. The 15 degree polynomial does come close, however. It might be worth setting up a way to test an extended range of polynomial degrees, however judging by the results above, increasing the degrees has a diminishing return and I suspect would begin to over-fit at some critical number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM:\n",
      "------------\n",
      "Precision: 0.844\n",
      "Recall: 0.843\n",
      "F1-Score: 0.843\n",
      "Accuracy: 0.843\n",
      "True Negative: 2185\tFalse Positive: 314\n",
      "False Negative: 472\tTrue Positive: 2029\n"
     ]
    }
   ],
   "source": [
    "test_lin_pred = svm_clf_lin.predict(X_test_final)\n",
    "print(\"Linear SVM:\\n------------\")\n",
    "test_result = get_score(test_set[\"sentiment\"], test_lin_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_set[\"sentiment\"], test_lin_pred).ravel()\n",
    "print(f\"True Negative: {tn}\\tFalse Positive: {fp}\\nFalse Negative: {fn}\\tTrue Positive: {tp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
